{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "pzC3nFCwTlzh",
    "outputId": "5c7a82f8-1274-4cbc-e088-4b298fd43afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Collecting xformers\n",
      "  Downloading https://download.pytorch.org/whl/cu128/xformers-0.0.34-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu128/xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu128/xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cu128/xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers\n",
      "Successfully installed xformers-0.0.33.post1\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2026.1.4-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2026.1.4 (from unsloth)\n",
      "  Downloading unsloth_zoo-2026.1.4-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.46.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-1.0.5-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.0.33.post1)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
      "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.12.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.18.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.6)\n",
      "Collecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth)\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n",
      "  Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (0.22.2)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2026.1.4->unsloth)\n",
      "  Downloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2026.1.4->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.4->unsloth) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2026.1.4->unsloth)\n",
      "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Downloading unsloth-2026.1.4-py3-none-any.whl (405 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m405.7/405.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2026.1.4-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-1.0.5-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, pyarrow, msgspec, tyro, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "Successfully installed bitsandbytes-0.49.1 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.20.0 pyarrow-23.0.0 torchao-0.15.0 trl-0.24.0 tyro-1.0.5 unsloth-2026.1.4 unsloth_zoo-2026.1.4\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.24.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install unsloth\n",
    "!pip install transformers\n",
    "!pip install --no-deps trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vc2_QfdTvReH",
    "outputId": "2b9275b7-d5ed-4204-d108-968cb5ad345b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "max_seq_length = 4096\n",
    "dtype = None ## Auto detect\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q70i2eChUNFl",
    "outputId": "8eb3bb6e-dceb-4731-b714-28be0e2ab64e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "f327a038c97743368bb4094c915f9a40",
      "018ae63d0d7a46f6b3e7f303dba51a5f",
      "93e9fc7b3a194c1ca67e2b439d10c709",
      "3e0b7e9813354a0e902683fb90a39749",
      "9325778a7a0b4fcfa8663a547b8467c3",
      "d756b1996be24276a630c11b8995165b",
      "1f309926179946918d48d9e898a3d03a",
      "169e6f167243423aa8025f2c7c6920e6",
      "105230201b074b25882158a891e52a55",
      "0f6215d2bfd043ef9acfb36eb992d7a5",
      "1515144a8eba494193fc1e195b979cf6",
      "27e248539bb341009fccc26894b565ce",
      "7773033dc1b64e29b978a304525c6991",
      "e958ccc3cca64c9fbf3b035aab3ebfc8",
      "0ca52cd94c9546c8a3d9b983f45281fd",
      "a1a45b1798ee40fbbd297ce224fc6d04",
      "f1bdbb5cff8240d6b04af2f81b627e9c",
      "1a2faa2618c24b279e017d9c42c70934",
      "3ab8685cda804c1b8a21c816a8211285",
      "15a8e0a473df4d0aaa0dc86450fb021b",
      "2a3af05a75844fbbae08e8ca28a761f8",
      "7792f268a8874274ab72106f556addd0",
      "1b305bd180794b1684000459b8bcf60f",
      "8098b9484a7f4db8825a1bdb97077615",
      "367ebb43eb364700ba65c7937cb20c9a",
      "0b07b1ec614c4815800978b9d41242d0",
      "f4ba80776b61479abc497a8305ff425a",
      "0f42377c450843eca6df0a790959ee93",
      "31d6c23724d6461891cd8a85d877ce3e",
      "8123dfcf488e45348c0b841c04b30bd6",
      "2efa077136f74f13822ed51f8bfb275a",
      "4c88e6fa49774d938e24e0c1e8f8e397",
      "9e7dd2fc24934ef0b32654d8388a2f0e",
      "4f18d90377c945ce90b0c1e833e14cfe",
      "bedfdf94c4a8470995963366e44d6c65",
      "37e7b0b0a68448c18b2c1f630516ef02",
      "9e50ee63bcb74613b164b3523ec048ce",
      "682a4aefc55b40079f4fce317614362b",
      "b013c7a333b24f8a86fa0270f1ec5b89",
      "8f35790cef0e4a24897ba0e2d27877ca",
      "ff645148b38f49f98ecab4791f4348d0",
      "3e142ae758754fceae429183b0ee623f",
      "ede1fc66247a418cb19c5e0483324c80",
      "6e9c689ad0fc4604854fcccaf4e0a20d",
      "f6f88119bec449afb15e6ecedbb95d3b",
      "047f64861c3640228162e852d28183be",
      "96880e20f8f046e69da04cec59abeca8",
      "879ad544a15a4844a35f1455accf1c78",
      "4a2f213609894074b022543fc04272b4",
      "795346421e6c4ae5af586b8519b50b45",
      "bc820ceee92245769de34ed5a54651cd",
      "5554191850aa4585a05666648bdad6ac",
      "cea29dd8005046929dcf07081a90a09a",
      "8a593a9eb72746c1aafe7d6bd5551b0b",
      "b6901de537214af6b4a23db6420023ee",
      "25dd390a4ed545dd854d16911ff87de6",
      "7c2dbe15bc0c47b0b835d9cf4dc36472",
      "2ec408108e7b49e58fd8dd72a565a7bc",
      "18c36f6731db44d19ae0151a3e13bc9e",
      "74c70a818c05472887a2440a4816156b",
      "bd47fe53e0c44b2ca483cc20d1b0f8f5",
      "4414b799b8824ed2a4af47fd05c1f727",
      "0bc11ef251654557a3c8be549fafbdba",
      "a4adfef505fc41209a09910004ad63d6",
      "64a28dc923e44fe08081a62a4425c86a",
      "441fcc0e7b2941828e3b1f1a9a416000"
     ]
    },
    "id": "KxhRHBQnUNC-",
    "outputId": "5cfbeed6-1a41-483c-e2ec-e3aec05df715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:unsloth_zoo.log:Unsloth: unsloth/tinyllama-bnb-4bit can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f327a038c97743368bb4094c915f9a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/762M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e248539bb341009fccc26894b565ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b305bd180794b1684000459b8bcf60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/948 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f18d90377c945ce90b0c1e833e14cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f88119bec449afb15e6ecedbb95d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dd390a4ed545dd854d16911ff87de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"unsloth/tinyllama-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtEk9Ua9YF3p",
    "outputId": "e1054aaf-1a55-4ae2-a194-4f53b9e2ff21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7pFiCgrYY12",
    "outputId": "f23317cf-a229-4291-b76f-66ec817df186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='unsloth/tinyllama-bnb-4bit', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4NXFL9uYZzy",
    "outputId": "0f6d1123-825c-4fbf-e537-7a2e55185c1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.4 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,            ## LoRA rank (8/16/32/64 are common)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,     ## Usually r or 2*r\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=False,\n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwXG65X1YF1Z",
    "outputId": "da197df6-ac2c-4bcd-fb43-78cc5e0e5e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25,231,360 || all params: 1,125,279,744 || trainable%: 2.2422\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEAxrkpOZpLK",
    "outputId": "8f47b85f-893f-47ba-8fc6-f7a60c539d30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: base_model.model.model.embed_tokens.weight | Shape: torch.Size([32000, 2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.0.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.0.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.1.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.1.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.2.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.2.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.3.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.3.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.4.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.4.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.5.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.5.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.6.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.6.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.7.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.7.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.8.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.8.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.9.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.9.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.10.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.10.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.11.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.11.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.12.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.12.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.13.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.13.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.14.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.14.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.15.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.15.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.16.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.16.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.17.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.17.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.18.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.18.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.19.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.19.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.20.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.20.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight | Shape: torch.Size([262144, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight | Shape: torch.Size([256, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight | Shape: torch.Size([2097152, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight | Shape: torch.Size([32, 2048])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight | Shape: torch.Size([5632, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight | Shape: torch.Size([5767168, 1])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight | Shape: torch.Size([32, 5632])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight | Shape: torch.Size([2048, 32])  |  Requires grad: True\n",
      "Layer name: base_model.model.model.layers.21.input_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.layers.21.post_attention_layernorm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.model.norm.weight | Shape: torch.Size([2048])  |  Requires grad: False\n",
      "Layer name: base_model.model.lm_head.weight | Shape: torch.Size([32000, 2048])  |  Requires grad: False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(f\"Layer name: {name} | Shape: {param.shape}  |  Requires grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mgZho8-VYFyz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xohnm4PJUNAc",
    "outputId": "a0311538-f45e-429c-e36a-8b3bedb5aaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 25231360\n",
      "Total parameters: 640837632\n",
      "Percent: 3.94%\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total trainable parameters: {trainable_params}\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Percent: {(trainable_params / total_params) * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oGjL674bNHE",
    "outputId": "46d9d47f-6b24-4911-cb60-17b0083f5e67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UfBlkmlXbTWt",
    "outputId": "e12aac3b-44b5-49b0-bc1d-fac1a91fed64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIL8un9pbe5b",
    "outputId": "4cd5ddf0-e4fe-4dac-cad2-c0644a26f4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(isinstance(model, PeftModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mbI6q2qby0o",
    "outputId": "082c75c0-fbf7-4bad-f1a0-fdc234b51b19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping={'base_model_class': 'LlamaForCausalLM', 'parent_library': 'transformers.models.llama.modeling_llama', 'unsloth_fixed': True}, peft_version='0.18.1', base_model_name_or_path='unsloth/tinyllama-bnb-4bit', revision=None, inference_mode=False, r=32, target_modules={'v_proj', 'o_proj', 'down_proj', 'up_proj', 'q_proj', 'gate_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qHy9V3dCb8Sc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "b216a09485cf47ccb2f7daa370c02e1e",
      "13f76ec143e044389b175b7423c9e76d",
      "15767e4a6c484e7f9da2e1161150302e",
      "4e00ca669cad40439fde7021c46ac6df",
      "2c203ec596b64bcfa606555e8137570f",
      "1d59fb75218d44a2bd6de2b7d9e76868",
      "72bf4a9449594027b76bb02909a3ad13",
      "e5c4b21524574c11b34641aa55a97e94",
      "c55f29ade2b04a66a267d1f363d57220",
      "d435fafb6659496ebaa6196962ed508d",
      "ef3a9036ac354f37949235ed29e2555e",
      "b5c45998897943ec8abfa25f9bbf046e",
      "88c10bffff6e441c85a8bf050cdea95d",
      "607473e084a241798ac931c058a8001f",
      "80f19773783d4ca2b9d93702ca2f1306",
      "905415d6d45a4c469cdd145a09adaa77",
      "881fe59222894094a667b86ee362e703",
      "9fb1d028bc564f7ca227c516633a4f59",
      "49a03a67a1da44b88e56cb114ca8822f",
      "ca46953ecfd4431ca782c27a83123887",
      "6df77f9857da466a9c7bc4ef8118d175",
      "b8086893269146daa6bab95af0b1a4ae",
      "e292aadb6ec64aae837527b9c77380a9",
      "3d84559706ba4529a302c724d336d8cc",
      "ce95cb08c6c54c45947f9192b62cc5c4",
      "a7dbdc7845404d2c85a20d6d75b241b7",
      "b2016b6468dd4abaa1a206500297b4cb",
      "5249d51e293c4e9587610d33f82defee",
      "d6c3f372393f461fb2446af6f4bb3d5a",
      "fceca900035a45818ce9199ea784acec",
      "a43b940ff675439f890cceac9f843bb2",
      "3521385c6b384de18d73f6e9b226c4c9",
      "6278320018074a8a8d5453f49ed87223",
      "bd1daf01cf374b9eaad35574663ba2e2",
      "db74aa0da42c43e2a2389073a1b93ccd",
      "68f19e6576cc45368ebfcf815b9de7bf",
      "1e7d1a75447b4924a8e7d8d37efe64af",
      "19a679d9f620494cb183a8dbfb5abd0c",
      "d3359aabee8e454792103c54b3f7cf55",
      "0df949a020234323ad7044b457e68ccb",
      "ea0e31b6cf9c4b2e93316a27a7fce9b8",
      "140bafb03343429ea4ea027ca9860e5c",
      "b1634d4ded02499080a4709dc3e3684a",
      "58a4fb1adc3f40e191ef631fb5a0e61e"
     ]
    },
    "id": "ELseMTgYb6Ym",
    "outputId": "fdb7237b-3749-4d3e-c4a2-b33fb8df2b09"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b216a09485cf47ccb2f7daa370c02e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c45998897943ec8abfa25f9bbf046e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e292aadb6ec64aae837527b9c77380a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1daf01cf374b9eaad35574663ba2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "alpaca_format = \"\"\"Below is the instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately comlpetes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "## Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def format_data(examples):\n",
    "  texts = []\n",
    "  for instructions, input_text, output in zip(\n",
    "      examples[\"instruction\"], examples[\"input\"], examples[\"output\"]\n",
    "  ):\n",
    "    text = alpaca_format.format(instructions, input_text, output) + eos_token\n",
    "    texts.append(text)\n",
    "  return {\"text\": texts}\n",
    "\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=123).select(range(750))\n",
    "\n",
    "dataset = dataset.map(format_data, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNqoBJcSch9D",
    "outputId": "d722428a-9caf-4379-d437-01155e30c29b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 750\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xqxGlJkSfDf0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BVhYbmIie_6E"
   },
   "outputs": [],
   "source": [
    "!pip install -q psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cbF5wUb2eril"
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=20,\n",
    "    seed=123,\n",
    "    output_dir=\"output\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a556d7f582d84455968a5263898772dd",
      "c10596f00e894dca9d221683b9cfbf70",
      "3674919bc79e4e7d8ae2e1213852aea2",
      "a03f8fc19b5c44bbb3f8638ba92c46cf",
      "3e5e808dee7643b5a0bd7b2206b39e26",
      "94439a84cf8e417f99ae79f04484cf4e",
      "a08c13e1af38468a9a1ddf7b7fdb37cd",
      "4a4bbcda881c4ba59900df068c01cece",
      "05141e5af9f64f69a439527e4cf03ed5",
      "7d83aa6be41e49778d99ed49fff518f6",
      "474b274a5b124f03940cedd530e9f0f8"
     ]
    },
    "id": "BnzU8j66g6_S",
    "outputId": "98fcc431-b888-4e78-e0c4-98f508d4520e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a556d7f582d84455968a5263898772dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kun7QwikfLtj"
   },
   "outputs": [],
   "source": [
    "import time, psutil\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "xNlGWBbwezp-",
    "outputId": "6750489b-c1f7-4380-f149-9f27e0b54720"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 750 | Num Epochs = 1 | Total steps = 47\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 25,231,360 of 1,125,279,744 (2.24% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.044400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process = psutil.Process()\n",
    "train_start_time = time.time()\n",
    "cpu_ram_before = process.memory_info().rss / (1024 * 1024 * 1024) ## in GB\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "train_end_time = time.time()\n",
    "cpu_ram_after = process.memory_info().rss / (1024 * 1024 * 1024)\n",
    "peak_gpu_vram_gb = round(torch.cuda.max_memory_reserved() / (1024**3), 3)\n",
    "cpu_ram_used_gb = round(cpu_ram_after - cpu_ram_before, 3)\n",
    "total_training_time_min = round((train_end_time - train_start_time) / 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IEeb5j2bJB5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sq7RLsTTbBhl",
    "outputId": "bd63d7d0-260b-4f95-c90f-96c7e12cbe3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time (in min): 3.307\n",
      "Peak GPU VRAM (in GB): 1.945\n",
      "CPU RAM Used (in GB): 0.849\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Time (in min): {total_training_time_min}\")\n",
    "print(f\"Peak GPU VRAM (in GB): {peak_gpu_vram_gb}\")\n",
    "print(f\"CPU RAM Used (in GB): {cpu_ram_used_gb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FK26e-Ja4Oc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skU0gWWNkPBq",
    "outputId": "2736280f-8ed8-471e-b04d-b95c3fe69c6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tinyllama_lora_model/tokenizer_config.json',\n",
       " 'tinyllama_lora_model/special_tokens_map.json',\n",
       " 'tinyllama_lora_model/tokenizer.model',\n",
       " 'tinyllama_lora_model/added_tokens.json',\n",
       " 'tinyllama_lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save = \"tinyllama_lora_model\"\n",
    "\n",
    "model.save_pretrained(model_save)\n",
    "tokenizer.save_pretrained(model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oDRZbzcm1g1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "dd1b43565bb243998fa44d2e7a3a79a6",
      "a36597d706ea4719975b3b4f4dc42b94",
      "b149bb9b7e664a5591e34ab6f3de515a",
      "b13031fa76cc4e2392de56a5ab7620be",
      "cffd03677fdb42bc86aa12b65886b2cd",
      "dc55c12d4b9a435ca98bca300235c69e",
      "478ed3b73fe44872aef30bf4b099606a",
      "6a8c94f1f9a74081856a2db087d72a70",
      "4ac49fd591704179ac003c89bf5e3224",
      "635c8a5c3470433b87bdf5e4f6f06de6",
      "d9f5864330d34256bbb2d42c6d019338",
      "4814cdae6c124e6fb15cb18a444d259f",
      "33107f79bb87437c9b3c21bc15c7e10f",
      "7ee8073c418f4c178be8017dfb166647",
      "7e6d293693ef4faea2cdd174f98cf189",
      "35a2f49adcba4f4ebc45a915ff8098b0",
      "17b8572d296a450cb4beecd072a78909",
      "269f793f2f29422fb3891df803afd2ac",
      "0ad2109eac6d43f68a3e0a9ec73711c3",
      "6acd850daf0e4dcabe9dbdb437fd205f",
      "b78951e1353c4d3da7939285c9dc35f0",
      "154147c7f90f48b8962050085f0660c9",
      "45fb635f7bb44d99bcfeb7414f499b3c",
      "950f3f4a62d4448a8e667e165111bd71",
      "e67092fc00eb4f9c8023309017450d9e",
      "7fd3794b853747cfacdc13de9dab4557",
      "f486905d0a9e45018a20938e7ed3beb6",
      "3a2bcd0000d644afa7529cf0fffec8fa",
      "4a1de28bcfe24bb3b615fd19dd64b351",
      "83b9bc69c20f4a36b475185e1c413f35",
      "9674fb136ed846ae9c6bc9969677f26b",
      "650217f0fdb24ece9fb2eac0e0e331c8",
      "8eb6f43ca884484bac702821998855b4",
      "2c1f7691be29411a9845f6bdecb15e24",
      "ea98f875d8d743489bdd6036aad96c98",
      "f0df78287fa640f2b8693df1d0327b2d",
      "ccffbb4025a543edb188a5807e8a8f56",
      "ac380a7b27324774b02aa75bb71adb00",
      "7d38fb1171244e49bfb7bcc591d26070",
      "0397e237a4a249d7b30b104c42f1ac43",
      "b81e3eb102a345ada0e7765c2cde5e4d",
      "c9c3b2e8e1964a37b3afc7834f8145f4",
      "295f1d8f87fa4ad4a60a3b14298b77e7",
      "27cfa699a30b476cbfb7f06f193426b2",
      "e7b1156cd0884d61ae23f7a8841d7ba0",
      "4d5860a1973a4762adf053d94ca99260",
      "7fba0453fc594c84ab41c29db367bf7b",
      "80b90d9307bc497087fe41774acca7b9",
      "c9cee7995e3d45b2a79a07a22adebc02",
      "8522b96e489b4072860fda483acd79f7",
      "82d5c3dc2ace4b52a1b5353f4800d9ad",
      "b57e5fcc790b4cbbba3fa10cc398f8e0",
      "dfd48236e66c417f8d1b1c9c002a259d",
      "0196beebe1234ece869aafc7f25c6e8d",
      "4d9e70646fdf4ac4b2db7273a24e9aeb",
      "b28cc8d9a81546dbb4db375fdfcfc898",
      "aa85d31943c649e18298063df227e19e",
      "42d912dd32d14a3d9cf40be812c579c6",
      "073fa44cecf847549bfbba6cf54b5600",
      "213e4459e1c847a7b294c447afe8fc9a",
      "e300e3d8e9b04ae1b54ad6647511f0d4",
      "c01e0da9032142d18ac6f97ea1c607a7",
      "3870df7e59614fcca50a5b45a5875ce8",
      "e1f3511f440948539f42f35ae32bf266",
      "3103b840ea68405e85471da5bf188f42",
      "3e909bcd5d09404f83dcadf9febc207b",
      "9542c218d3af4af5bed8c5b9077f1b57",
      "052e89adf6be46ab927c739e61bb49d5",
      "dbc93cb1e7b14af3927867185f49437a",
      "c92bd10d008647108ad8ebe862fa530c",
      "8ee95c2154084e01a4ada90b9535c3f3",
      "d71bc81c78a947aa992db8521f581a84",
      "2aace19b0d7d4159ba623bbb8e8ecfa0",
      "2750c7acdf9645c19f73ed4795385ec9",
      "04ca99e1790f472eb92afb1b716da890",
      "f43d45bcc74f443981c4c36737f27f54",
      "a4a1f06ab7974efdb22e6e006a073029"
     ]
    },
    "id": "JHCQwEjzm1eV",
    "outputId": "907aae9a-e48a-419d-fba6-f5eefc7325fc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1b43565bb243998fa44d2e7a3a79a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/544 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4814cdae6c124e6fb15cb18a444d259f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fb635f7bb44d99bcfeb7414f499b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1f7691be29411a9845f6bdecb15e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...adapter_model.safetensors:   1%|          | 1.00MB /  101MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/tinyllama_lora_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b1156cd0884d61ae23f7a8841d7ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28cc8d9a81546dbb4db375fdfcfc898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9542c218d3af4af5bed8c5b9077f1b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ora_model/tokenizer.model: 100%|##########|  500kB /  500kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(\"tinyllama_lora_model\")\n",
    "tokenizer.push_to_hub(\"tinyllama_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-nJ9-JykO8y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
