{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pF5qfj_bkbH-"
      ],
      "authorship_tag": "ABX9TyPnllXnTWwxuDk11wj96wMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitKadam/ColabNotebooks/blob/main/Self_Attention_%26_Multi_Head_Attention_with_Trainable_Weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SELF ATTENTION**"
      ],
      "metadata": {
        "id": "pF5qfj_bkbH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB6bntov6mWp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "input = torch.tensor(\n",
        "    [[0.72, 0.45, 0.31],   ## Dream\n",
        "     [0.75, 0.20, 0.55],   ## big\n",
        "     [0.30, 0.80, 0.40],   ## and\n",
        "     [0.85, 0.35, 0.60],   ## work\n",
        "     [0.55, 0.15, 0.75],   ## for\n",
        "     [0.20, 0.20, 0.85]]   ## it\n",
        ")\n",
        "\n",
        "words = [\"Dream\", \"big\", \"and\", \"work\", \"for\", \"it\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = input[1]\n",
        "print(x_2)\n",
        "d_in = input.shape[1]\n",
        "print(d_in)\n",
        "d_out = 2  ## Dimensionality of Context Vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3qQ_-nu6oZN",
        "outputId": "8d8f6001-339b-4a5a-f47b-16cc22620cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7500, 0.2000, 0.5500])\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "ZdSVXnD56oWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"W_query:\")\n",
        "print(W_query)\n",
        "print()\n",
        "print(\"W_key:\")\n",
        "print(W_key)\n",
        "print()\n",
        "print(\"W_out:\")\n",
        "print(W_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgpFCz6l6oR8",
        "outputId": "66e6f514-79bc-4342-befb-eaf6283b7111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query:\n",
            "Parameter containing:\n",
            "tensor([[ 0.3607, -0.2859],\n",
            "        [-0.3938,  0.2429],\n",
            "        [-1.3833, -2.3134]])\n",
            "\n",
            "W_key:\n",
            "Parameter containing:\n",
            "tensor([[-0.3172, -0.8660],\n",
            "        [ 1.7482, -0.2759],\n",
            "        [-0.9755,  0.4790]])\n",
            "\n",
            "W_out:\n",
            "Parameter containing:\n",
            "tensor([[-2.3652, -0.8047],\n",
            "        [ 0.6587, -0.2586],\n",
            "        [-0.2510,  0.4770]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2FuOiRp-fiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = torch.matmul(x_2, W_query)\n",
        "key_2 = torch.matmul(x_2, W_key)\n",
        "value_2 = torch.matmul(x_2, W_value)"
      ],
      "metadata": {
        "id": "T68Gzl_D-ffp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"query_2:\")\n",
        "print(query_2)\n",
        "print()\n",
        "print(\"key_2:\")\n",
        "print(key_2)\n",
        "print()\n",
        "print(\"value_2:\")\n",
        "print(value_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c50voVD1-fdo",
        "outputId": "129080c2-9842-4c59-96f0-93366e898cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_2:\n",
            "tensor([-0.5691, -1.4382])\n",
            "\n",
            "key_2:\n",
            "tensor([-0.4248, -0.4413])\n",
            "\n",
            "value_2:\n",
            "tensor([-1.7802, -0.3929])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyRHbKTt-fbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Q, K & V using x, Wq, Wk, Wv"
      ],
      "metadata": {
        "id": "9pCjx-nt_Frm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = torch.matmul(input, W_key)\n",
        "queries = torch.matmul(input, W_query)\n",
        "values = torch.matmul(input, W_value)"
      ],
      "metadata": {
        "id": "8rA8W8GA-fZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"keys:\")\n",
        "print(keys)\n",
        "print(\"Keys shape:\", keys.shape)\n",
        "print()\n",
        "print(\"queries:\")\n",
        "print(queries)\n",
        "print(\"Queries shape:\", queries.shape)\n",
        "print()\n",
        "print(\"values:\")\n",
        "print(values)\n",
        "print(\"Values shape:\", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6hZim-Y-fW5",
        "outputId": "74e1ec0e-44b6-4610-a31b-5857f6a317b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys:\n",
            "tensor([[ 0.2559, -0.5992],\n",
            "        [-0.4248, -0.4413],\n",
            "        [ 0.9132, -0.2889],\n",
            "        [-0.2430, -0.5453],\n",
            "        [-0.6438, -0.1585],\n",
            "        [-0.5430,  0.1788]])\n",
            "Keys shape: torch.Size([6, 2])\n",
            "\n",
            "queries:\n",
            "tensor([[-0.3464, -0.8137],\n",
            "        [-0.5691, -1.4382],\n",
            "        [-0.7602, -0.8168],\n",
            "        [-0.6613, -1.5461],\n",
            "        [-0.8982, -1.8559],\n",
            "        [-1.1825, -1.9750]])\n",
            "Queries shape: torch.Size([6, 2])\n",
            "\n",
            "values:\n",
            "tensor([[-1.4843, -0.5479],\n",
            "        [-1.7802, -0.3929],\n",
            "        [-0.2829, -0.2575],\n",
            "        [-1.9304, -0.4883],\n",
            "        [-1.3903, -0.1236],\n",
            "        [-0.5546,  0.1928]])\n",
            "Values shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzCrUFNO6oP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keys corresponding to second token (x2) and the attention of second token to itself"
      ],
      "metadata": {
        "id": "pT9_C1jqAwxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Keys corresponding to second token (x2):\")\n",
        "print(key_2)\n",
        "print()\n",
        "print(\"Attention of second token to itself:\")\n",
        "attn_score_22 = torch.matmul(query_2, key_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsmgLxi6AL9G",
        "outputId": "dc80a99e-0172-4422-db63-e09f3ba2e7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys corresponding to second token (x2):\n",
            "tensor([-0.4248, -0.4413])\n",
            "\n",
            "Attention of second token to itself:\n",
            "tensor(0.8764)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8L6z9iEBXuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Attention Scores for query_2 (meaning for \"big\" (x2))"
      ],
      "metadata": {
        "id": "JqCGM5UUB2ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_score_2 = torch.matmul(query_2, keys.T)\n",
        "print(\"Attention Scores for query_2 (meaning for 'big'):\")\n",
        "print(attn_score_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEknnRR5AL51",
        "outputId": "2c447a77-6348-4a19-8783-df97ecebd7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores for query_2 (meaning for 'big'):\n",
            "tensor([ 0.7162,  0.8764, -0.1042,  0.9226,  0.5943,  0.0519])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jr-XF9qrAL3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Attention Score"
      ],
      "metadata": {
        "id": "NUFSPEArC4HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_score = torch.matmul(queries, keys.T)\n",
        "print(\"Complete Attention Score:\")\n",
        "print(attn_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saEgyJd_ALym",
        "outputId": "a1585ccb-4d01-4b4d-842a-0cf7168f85d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete Attention Score:\n",
            "tensor([[ 0.3989,  0.5062, -0.0812,  0.5279,  0.3520,  0.0426],\n",
            "        [ 0.7162,  0.8764, -0.1042,  0.9226,  0.5943,  0.0519],\n",
            "        [ 0.2949,  0.6833, -0.4582,  0.6302,  0.6189,  0.2667],\n",
            "        [ 0.7572,  0.9631, -0.1572,  1.0038,  0.6708,  0.0827],\n",
            "        [ 0.8822,  1.2005, -0.2841,  1.2303,  0.8724,  0.1559],\n",
            "        [ 0.8808,  1.3738, -0.5092,  1.3644,  1.0743,  0.2890]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N46QygGHALv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale by 1/sqrt(d) and take softmax"
      ],
      "metadata": {
        "id": "kobHHEdXDbBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_score_2 / (d_k ** 0.5), dim=-1)\n",
        "\n",
        "print(\"d value:\")\n",
        "print(d_k)\n",
        "print()\n",
        "print(\"Attention Weights for 'big':\")\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnnYGs38ALtu",
        "outputId": "513ee1b9-df06-49c8-fb01-f09d34d269eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d value:\n",
            "2\n",
            "\n",
            "Attention Weights for 'big':\n",
            "tensor([0.1859, 0.2082, 0.1041, 0.2151, 0.1705, 0.1162])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_score / (d_k ** 0.5), dim=-1)\n",
        "print(\"Attention Weights:\")\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wZERZh2ALpO",
        "outputId": "b4d68ed4-d929-4def-828a-a3d9ded926e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            "tensor([[0.1776, 0.1916, 0.1265, 0.1945, 0.1718, 0.1380],\n",
            "        [0.1859, 0.2082, 0.1041, 0.2151, 0.1705, 0.1162],\n",
            "        [0.1560, 0.2054, 0.0916, 0.1978, 0.1962, 0.1530],\n",
            "        [0.1841, 0.2129, 0.0964, 0.2191, 0.1732, 0.1143],\n",
            "        [0.1798, 0.2252, 0.0788, 0.2300, 0.1786, 0.1076],\n",
            "        [0.1666, 0.2360, 0.0623, 0.2345, 0.1910, 0.1096]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFT8TwunALm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context Vector"
      ],
      "metadata": {
        "id": "T2rlBv3FFkIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = torch.matmul(attn_weights_2, values)\n",
        "print(\"Context Vector for 'big':\")\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2b6q3ciFkBu",
        "outputId": "f15f5778-ad79-4525-f97f-2d295001b2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector for 'big':\n",
            "tensor([-1.3927, -0.3141])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = torch.matmul(attn_weights, values)\n",
        "print(\"Context Vector:\")\n",
        "print(context_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6GJFFGoFj_n",
        "outputId": "f766b230-4936-4531-e1ce-b70c1259cc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[-1.3314, -0.2947],\n",
            "        [-1.3927, -0.3141],\n",
            "        [-1.3626, -0.2811],\n",
            "        [-1.4067, -0.3157],\n",
            "        [-1.4420, -0.3209],\n",
            "        [-1.4640, -0.3170]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1g_GuWN-Fj9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
        "    self.W_key = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
        "    self.W_value = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = torch.matmul(x, self.W_key)\n",
        "    queries = torch.matmul(x, self.W_query)\n",
        "    values = torch.matmul(x, self.W_value)\n",
        "\n",
        "    attn_scores = torch.matmul(queries, keys.T)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** (0.5)), dim=-1)\n",
        "\n",
        "    context_vec = torch.matmul(attn_weights, values)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "Ob9VbeVhFj7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "sa_v1 = SelfAttention_v1(d_in=3, d_out=2)\n",
        "context_vec = sa_v1(input)\n",
        "print(\"Context Vector:\")\n",
        "print(context_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VYvzRpoFj43",
        "outputId": "c98c897a-797b-45a9-b90e-dcf565dcffb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[-1.3314, -0.2947],\n",
            "        [-1.3927, -0.3141],\n",
            "        [-1.3626, -0.2811],\n",
            "        [-1.4067, -0.3157],\n",
            "        [-1.4420, -0.3209],\n",
            "        [-1.4640, -0.3170]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1f3ijBqcFj2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = torch.matmul(queries, keys.T)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** (0.5)), dim=-1)\n",
        "\n",
        "    context_vec = torch.matmul(attn_weights, values)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "QEnfj1FwIw-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "sa_v2 = SelfAttention_v2(d_in=3, d_out=2, qkv_bias=True)\n",
        "context_vec = sa_v2(input)\n",
        "print(\"Context Vector:\")\n",
        "print(context_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7mUHAZIw7_",
        "outputId": "ac4afb9f-4670-4293-8d1a-9c46ebb16610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[0.3103, 0.5039],\n",
            "        [0.3111, 0.5033],\n",
            "        [0.3092, 0.5047],\n",
            "        [0.3112, 0.5032],\n",
            "        [0.3112, 0.5033],\n",
            "        [0.3106, 0.5037]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mA6u3_YjIw34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MULTI-HEAD ATTENTION**"
      ],
      "metadata": {
        "id": "UympXpMakjRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input = torch.tensor(\n",
        "    [[0.72, 0.45, 0.31],   ## Dream\n",
        "     [0.75, 0.20, 0.55],   ## big\n",
        "     [0.30, 0.80, 0.40],   ## and\n",
        "     [0.85, 0.35, 0.60],   ## work\n",
        "     [0.55, 0.15, 0.75],   ## for\n",
        "     [0.20, 0.20, 0.85]]   ## it\n",
        ")\n",
        "\n",
        "words = [\"Dream\", \"big\", \"and\", \"work\", \"for\", \"it\"]"
      ],
      "metadata": {
        "id": "voAuYtGpkuR3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import forward_ad\n",
        "import torch.nn as nn\n",
        "\n",
        "class CasualAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.d_in = d_in\n",
        "    self.context_length = context_length\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape   ## Batch Dimension b\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = torch.matmul(queries, keys.transpose(1, 2))  ## Here we take 1 and 2 because 0th dimension is Batch Dimension\n",
        "\n",
        "    attn_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "    )\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
        "\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = torch.matmul(attn_weights, values)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "rmjF2zg6kuPX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in = input.shape[-1]\n",
        "d_out = 2\n",
        "batch = torch.stack((input, input), dim=0)\n",
        "print(batch)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1BmPK42kuMn",
        "outputId": "0daf095b-2abb-473c-eb33-fcfab8c3ad84"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.7200, 0.4500, 0.3100],\n",
            "         [0.7500, 0.2000, 0.5500],\n",
            "         [0.3000, 0.8000, 0.4000],\n",
            "         [0.8500, 0.3500, 0.6000],\n",
            "         [0.5500, 0.1500, 0.7500],\n",
            "         [0.2000, 0.2000, 0.8500]],\n",
            "\n",
            "        [[0.7200, 0.4500, 0.3100],\n",
            "         [0.7500, 0.2000, 0.5500],\n",
            "         [0.3000, 0.8000, 0.4000],\n",
            "         [0.8500, 0.3500, 0.6000],\n",
            "         [0.5500, 0.1500, 0.7500],\n",
            "         [0.2000, 0.2000, 0.8500]]])\n",
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_length, num_heads, dropout, qkv_bias):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "BP0youoWkuE5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "d_in = input.shape[-1]\n",
        "d_out = 2\n",
        "\n",
        "batch = torch.stack((input, input), dim=0)  ## Here batch size is 2\n",
        "context_length = batch.shape[1]\n",
        "\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, num_heads=3, dropout=0.1, qkv_bias=True)"
      ],
      "metadata": {
        "id": "gBcn4N4NkuCn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = mha(batch)\n",
        "print(\"Context Vector:\")\n",
        "print(context_vec)\n",
        "print(\"Context Vector Shape:\")\n",
        "print(context_vec.shape)"
      ],
      "metadata": {
        "id": "EWYGa7wHIw1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea524532-2d5e-4efa-ed25-655cf56337d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[[ 0.0000,  0.0000, -0.7784, -0.2237,  1.2173,  0.1742],\n",
            "         [ 0.4949,  0.4700, -0.6811, -0.1936,  1.2268,  0.1584],\n",
            "         [ 0.4578,  0.4371, -0.7688, -0.2008,  0.7948,  0.0513],\n",
            "         [ 0.4724,  0.4533, -0.7448, -0.2051,  1.2416,  0.1103],\n",
            "         [ 0.4199,  0.5063, -0.5164, -0.1443,  0.7687,  0.0858],\n",
            "         [ 0.3511,  0.4237, -0.4388, -0.0960,  1.1969,  0.0722]],\n",
            "\n",
            "        [[ 0.5688,  0.3726, -0.7784, -0.2237,  1.2173,  0.1742],\n",
            "         [ 0.4949,  0.4700, -0.6811, -0.1936,  1.2268,  0.1584],\n",
            "         [ 0.4578,  0.4371, -0.7688, -0.2008,  1.1989,  0.1091],\n",
            "         [ 0.4724,  0.4533, -0.7448, -0.2051,  1.2416,  0.1103],\n",
            "         [ 0.2964,  0.2521, -0.6005, -0.1651,  1.2282,  0.0973],\n",
            "         [ 0.1917,  0.2989, -0.6771, -0.1637,  0.8107,  0.0428]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "Context Vector Shape:\n",
            "torch.Size([2, 6, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmDqExNZrUa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "d_in = input.shape[-1]\n",
        "d_out = 2\n",
        "\n",
        "batch = torch.stack((input, ), dim=0)  ## Here batch size is 1\n",
        "context_length = batch.shape[1]\n",
        "\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, num_heads=3, dropout=0.1, qkv_bias=True)"
      ],
      "metadata": {
        "id": "DhuwNzpBrUYh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = mha(batch)\n",
        "print(\"Context Vector:\")\n",
        "print(context_vec)\n",
        "print(\"Context Vector Shape:\")\n",
        "print(context_vec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgHUulXjrUWZ",
        "outputId": "43d84feb-2ea0-4e98-d80d-2bdab8fa63d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[[ 0.0000,  0.0000, -0.7784, -0.2237,  1.2173,  0.1742],\n",
            "         [ 0.4949,  0.4700, -0.6811, -0.1936,  1.2268,  0.1584],\n",
            "         [ 0.4578,  0.4371, -0.7688, -0.2008,  1.1989,  0.1091],\n",
            "         [ 0.4724,  0.4533, -0.7448, -0.2051,  1.2416,  0.1103],\n",
            "         [ 0.4199,  0.5063, -0.4810, -0.1316,  1.0136,  0.0963],\n",
            "         [ 0.3511,  0.4237, -0.3001, -0.0828,  0.7902,  0.0196]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "Context Vector Shape:\n",
            "torch.Size([1, 6, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vcn3MQ2MrUUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "d_in = input.shape[-1]\n",
        "d_out = 2\n",
        "\n",
        "batch = torch.stack((input, ), dim=0)  ## Here batch size is 1\n",
        "context_length = batch.shape[1]\n",
        "\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, num_heads=10, dropout=0.3, qkv_bias=True)"
      ],
      "metadata": {
        "id": "CPBrOf_ArURq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = mha(batch)\n",
        "print(\"Context Vector:\")\n",
        "print(context_vec)\n",
        "print(\"Context Vector Shape:\")\n",
        "print(context_vec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqZNQa9wsGR3",
        "outputId": "c7643171-05d0-4465-ffcb-099fab16753d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[[ 0.0000,  0.0000, -1.0008, -0.2876,  0.0000,  0.0000, -0.4933,\n",
            "          -0.5937,  0.0000,  0.0000, -0.0992,  0.9979,  0.7522, -0.9791,\n",
            "           0.2575, -0.6159, -0.1867, -0.7596, -0.3448, -0.9653],\n",
            "         [ 0.6363,  0.6043, -0.8757, -0.2490,  1.5773,  0.2036, -0.4279,\n",
            "          -0.7763,  0.3010,  0.3950, -0.1434,  1.0314,  0.3667, -0.4773,\n",
            "           0.0594, -0.8003, -0.2404, -0.6143, -0.1270, -0.6795],\n",
            "         [ 0.4227,  0.4015, -0.9884, -0.2582,  0.5195,  0.0743, -0.2724,\n",
            "          -0.4824,  0.3355,  0.2783, -0.2392,  1.0062,  0.6148, -0.6874,\n",
            "           0.0867, -0.2074, -0.2970, -0.7485, -0.1171, -0.3277],\n",
            "         [ 0.1248,  0.1208, -0.9575, -0.2637,  1.2608,  0.1403, -0.1082,\n",
            "          -0.2462,  0.3563,  0.3711, -0.0497,  0.5579,  0.3758, -0.5190,\n",
            "           0.1020, -0.3012, -0.2596, -0.4883, -0.3347, -1.0533],\n",
            "         [ 0.4397,  0.5540, -0.4914, -0.1289,  1.2206,  0.0962, -0.3502,\n",
            "          -0.7201,  0.2273,  0.2372, -0.2233,  0.8186,  0.6101, -0.8003,\n",
            "          -0.0706, -0.4091, -0.3386, -0.6282, -0.1723, -0.7509],\n",
            "         [ 0.3602,  0.6387, -0.7608, -0.1884,  1.2731,  0.0620, -0.2495,\n",
            "          -0.4096,  0.3722,  0.3873, -0.1644,  0.4880,  0.8402, -0.9838,\n",
            "          -0.0291, -0.5853, -0.3188, -0.5034, -0.2087, -0.7781]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "Context Vector Shape:\n",
            "torch.Size([1, 6, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOAAjK1IsGPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
