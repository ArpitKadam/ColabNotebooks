{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCHRgpBNu3sYEYEoNEySmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitKadam/Attention-Is-All-You-Code/blob/main/LLM-from-Scratch/CHP_11_LLM_performance_on_real_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREVIOUSLY**"
      ],
      "metadata": {
        "id": "_Wb2RKrJnfBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\":768,\n",
        "    \"n_heads\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"drop_rate\":0.1,\n",
        "    \"qkv_bias\":False\n",
        "}\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
        "    ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, drop_rate, num_heads, qkv_bias):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "    self.register_buffer(\"simple_mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch, num_tokens, d_in = x.shape\n",
        "\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    attn_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
        "\n",
        "    attn_scores.masked_fill_(\n",
        "        self.simple_mask.bool()[:num_tokens, :num_tokens],\n",
        "        -torch.inf\n",
        "    )\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
        "\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = torch.matmul(attn_weights, values)\n",
        "\n",
        "    context_vec = context_vec.transpose(1, 2)\n",
        "\n",
        "    context_vec = context_vec.contiguous().view(batch, num_tokens, self.d_out)\n",
        "\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.attn = MultiHeadAttention(\n",
        "        d_in = cfg['emb_dim'],\n",
        "        d_out = cfg['emb_dim'],\n",
        "        context_length = cfg['context_length'],\n",
        "        num_heads = cfg['n_heads'],\n",
        "        qkv_bias = cfg['qkv_bias'],\n",
        "        drop_rate = cfg['drop_rate']\n",
        "      )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2 = LayerNorm(cfg['emb_dim'])\n",
        "    self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    ## Connection for attention block\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.attn(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    ## Conection for feed forward block\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x\n",
        "\n",
        "class GPT_MODEL(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "    self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "    self.drop_out = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
        "    )\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg['emb_dim'])\n",
        "    self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_length = x.shape\n",
        "\n",
        "    token_embeddings = self.token_emb(x)\n",
        "    pos_embeddings = self.pos_emb(torch.arange(seq_length, device=x.device))\n",
        "\n",
        "    x = token_embeddings + pos_embeddings\n",
        "    x = self.drop_out(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "3ePXXLP_kHrN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "\n",
        "    idx_crop = idx[:, -context_size:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_crop)\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "  return idx\n",
        "\n",
        "model = GPT_MODEL(GPT_CONFIG_124M)\n",
        "model.eval()        ## Use to disable dropout during inference\n",
        "\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  token_list = token_ids.squeeze(0).tolist()\n",
        "  decoded_text = tokenizer.decode(token_list)\n",
        "  return decoded_text"
      ],
      "metadata": {
        "id": "vBUiNeTwkHo-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8co5RSm4kHme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPLEMENTING THE DATALOADERS**"
      ],
      "metadata": {
        "id": "qXZP8QQu0iUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/the-verdict.txt\", \"r\") as file:\n",
        "  text_data = file.read()"
      ],
      "metadata": {
        "id": "PVYiwu_XkHj-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75bW_6spzmmj",
        "outputId": "d1c7db8d-c482-4946-9f95-c260d41f6864"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
            "\n",
            "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
            "\n",
            "Well!--even through th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_chars = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(f\"Total Characters: {total_chars}\")\n",
        "print(f\"Total Tokens: {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuDW-KnQzqHV",
        "outputId": "591b2b91-d4bf-4a79-8d8b-c096be94f07b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters: 20479\n",
            "Total Tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataSetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      ## Here max_length is the context_size\n",
        "      input_chunk = token_ids[i: i + max_length]\n",
        "      target_chunk = token_ids[i+1: i + max_length + 1]\n",
        "\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.target_ids[index]\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset = GPTDataSetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "lxwvTWDh0rdw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.80\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(train_data, batch_size=2, max_length=256, stride=256)\n",
        "val_loader = create_dataloader_v1(val_data, batch_size=2, max_length=256, stride=256)"
      ],
      "metadata": {
        "id": "pPwrgRPz2KK7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Loader:\")\n",
        "for x, y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nVal Loader:\")\n",
        "for x, y in val_loader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7nEmztc2G9u",
        "outputId": "bec1cc91-4102-441f-f85a-13d202bb3530"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Val Loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn-vrb264yXS",
        "outputId": "077171c3-2a36-487b-90e5-4ed2cadd5add"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "  train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "  val_tokens += input_batch.numel()\n",
        "\n",
        "print(f\"Train Tokens: {train_tokens}\")\n",
        "print(f\"Val Tokens: {val_tokens}\")\n",
        "print(f\"All Tokens: {train_tokens + val_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waKDgKfL5W2d",
        "outputId": "7818c6e5-44d8-4209-cc70-a39ddf876ef1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Tokens: 4096\n",
            "Val Tokens: 1024\n",
            "All Tokens: 5120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LR_aWylq-28b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPLEMENTING A LOSS CALCULATION FUNCTION**"
      ],
      "metadata": {
        "id": "TGFv9VAy-pS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "def cal_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "\n",
        "  else:\n",
        "    ## Reduce the number of batches to match the total number of batches in dataloader\n",
        "    ## i.e if num_batches exceeds the number of the batches in data_loader\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if batch_idx < num_batches:\n",
        "      loss = cal_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "WC-CzH0c4u2M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G3WcNoY8-5ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING [A SINGLE FORWARD PASS]**"
      ],
      "metadata": {
        "id": "c_Jt7uRY-6Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPT_MODEL(GPT_CONFIG_124M).to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "  loss_train = cal_loss_loader(train_loader, model, device)\n",
        "  loss_val = cal_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(f\"Train Loss: {loss_train}\")\n",
        "print(f\"Val Loss: {loss_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqTwjRBP19-H",
        "outputId": "93f8293d-b507-4424-eb31-a0a921ad0406"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 11.00752067565918\n",
            "Val Loss: 11.008864879608154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvB80pYe1z4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}