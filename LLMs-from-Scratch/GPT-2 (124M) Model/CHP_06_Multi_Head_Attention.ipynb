{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQ9sjMSSu/wsHHEN/jTa7E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitKadam/Attention-Is-All-You-Code/blob/main/LLM-from-Scratch/CHP_06_Multi_Head_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MULTI-HEAD ATTENTION**"
      ],
      "metadata": {
        "id": "qmUWp1NclnF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extending Single Head Attention to Multi-Head Attention"
      ],
      "metadata": {
        "id": "SvO4-YdolvlT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HoFwyTJPEJzM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.context_length = context_length\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.register_buffer(\"simple_mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, d_in = x.shape\n",
        "\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
        "\n",
        "    attn_scores.masked_fill_(\n",
        "        self.simple_mask.bool()[:num_tokens, :num_tokens],\n",
        "        -torch.inf\n",
        "    )\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
        "\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = torch.matmul(attn_weights, values)\n",
        "\n",
        "    return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h38vEh3KlmvR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "FtGgF7velmtB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqPE8MJklmqi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input = torch.tensor(\n",
        "    [[0.72, 0.45, 0.31],   ## Dream\n",
        "     [0.75, 0.20, 0.55],   ## big\n",
        "     [0.30, 0.80, 0.40],   ## and\n",
        "     [0.85, 0.35, 0.60],   ## work\n",
        "     [0.55, 0.15, 0.75],   ## for\n",
        "     [0.20, 0.20, 0.85]]   ## it\n",
        ")\n",
        "\n",
        "words = [\"Dream\", \"big\", \"and\", \"work\", \"for\", \"it\"]\n",
        "\n",
        "batch = torch.stack([input, input], dim=0)\n",
        "print(batch.shape)\n",
        "d_in = batch.shape[-1]\n",
        "d_out = 2\n",
        "context_length = batch.shape[1]\n",
        "dropout = 0.5\n",
        "qkv_bias = True\n",
        "num_heads = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PDNXMxtncrQ",
        "outputId": "8f4f75c7-84f9-4578-8171-5d7401daeba3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mh_attn = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
        "\n",
        "context_vector = mh_attn(batch)"
      ],
      "metadata": {
        "id": "peyP4s0cnz1d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Context Vector:\")\n",
        "print(context_vector)\n",
        "print(\"Shape:\", context_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GQxUZQPlmoS",
        "outputId": "70b80293-9ba6-4769-e340-7dfafe691154"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2077e+00,  1.1254e+00],\n",
            "         [ 1.4238e-01, -2.7253e-02, -9.7262e-01,  6.7575e-01,  2.5844e-01,\n",
            "           5.7308e-01,  3.8498e-01, -7.1997e-01, -5.9959e-01,  5.5871e-01],\n",
            "         [ 3.5423e-01,  3.5294e-02, -1.6565e+00,  1.3933e+00,  1.0789e-01,\n",
            "           3.2194e-01,  5.6686e-01, -9.1626e-01, -1.0933e+00,  7.1606e-01],\n",
            "         [ 0.0000e+00,  0.0000e+00, -1.3760e+00,  1.0847e+00,  3.5426e-01,\n",
            "           8.9170e-01,  2.8736e-01, -3.1735e-01, -8.4044e-01,  5.4209e-01],\n",
            "         [ 1.8945e-01,  3.4561e-02, -1.1860e+00,  7.8923e-01,  0.0000e+00,\n",
            "           0.0000e+00,  8.9272e-01, -1.3910e+00, -5.0857e-01,  5.1673e-01],\n",
            "         [ 1.3407e-01,  3.3340e-02, -8.1284e-01,  6.8262e-01,  0.0000e+00,\n",
            "           0.0000e+00,  6.0972e-01, -9.1083e-01, -5.6090e-01,  4.4197e-01]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2077e+00,  1.1254e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00, -9.7262e-01,  6.7575e-01,  0.0000e+00,\n",
            "           0.0000e+00,  8.2633e-01, -1.3340e+00,  0.0000e+00,  0.0000e+00],\n",
            "         [ 9.7012e-02, -1.8570e-02, -4.7196e-01,  4.4732e-01,  1.6561e-01,\n",
            "           3.6724e-01,  5.6686e-01, -9.1626e-01, -1.4977e+00,  1.0929e+00],\n",
            "         [ 9.5584e-02,  1.5563e-03, -4.9096e-01,  3.7842e-01,  3.5426e-01,\n",
            "           8.9170e-01,  4.7976e-01, -6.7717e-01, -6.3374e-01,  6.4391e-01],\n",
            "         [ 1.3152e-01,  4.5648e-02, -1.3987e+00,  1.0514e+00,  1.0067e-01,\n",
            "           3.0081e-01,  3.4076e-01, -6.1622e-01, -1.4726e+00,  1.2253e+00],\n",
            "         [ 4.1618e-02,  5.1852e-02, -9.0613e-01,  5.9142e-01,  2.3292e-01,\n",
            "           5.2921e-01,  7.2859e-01, -1.1603e+00, -7.6153e-01,  6.7782e-01]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "Shape: torch.Size([2, 6, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_uqKX0B3omGh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS**"
      ],
      "metadata": {
        "id": "8xgBQMq_sizt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    ## Lets just say we take d_out = 6, batch_size = 2, d_out = 6, num_heads = 2, d_in = 6, num_tokens = 3\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads   ## Reduce the projection dim to match the desired output dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)    ## Shape: (6, 6)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)      ## Shape: (6, 6)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)    ## Shape: (6, 6)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)    ## Linear Layer to combine head outputs\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"simple_mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch, num_tokens, d_in = x.shape      ## Shape: (2, 3, 6)\n",
        "\n",
        "    keys = self.W_key(x)                 ## Shape: (2, 3, 6)\n",
        "    queries = self.W_query(x)            ## Shape: (2, 3, 6)\n",
        "    values = self.W_value(x)             ## Shape: (2, 3, 6)\n",
        "\n",
        "    ## We implicitly split the matrix by adding a \"num_heads\" dimension\n",
        "    ## Unroll last dim: (batch, num_tokens, d_out) -> (batch, num_tokens, num_heads, head_dim)\n",
        "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)              ## Shape: (2, 3, 2, 3)\n",
        "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)        ## Shape: (2, 3, 2, 3)\n",
        "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)          ## Shape: (2, 3, 2, 3)\n",
        "\n",
        "    ## Transpose: (batch, num_tokens, num_heads, head_dim) -> (batch, num_heads, num_tokens, head_dim)\n",
        "    keys = keys.transpose(1, 2)            ## Shape: (2, 2, 3, 3)    (batch, num_heads, num_tokens, head_dim)\n",
        "    queries = queries.transpose(1, 2)      ## Shape: (2, 2, 3, 3)    (batch, num_heads, num_tokens, head_dim)\n",
        "    values = values.transpose(1, 2)        ## Shape: (2, 2, 3, 3)    (batch, num_heads, num_tokens, head_dim)\n",
        "\n",
        "    ## Calculate attention scores with Causal Mask\n",
        "    ## (batch, num_heads, num_tokens, head_dim) * (batch, num_heads, head_dim, num_tokens) = (batch, num_heads, num_tokens, num_tokens)\n",
        "    ## Shape: (2, 2, 3, 3) * (2, 2, 3, 3) = (2, 2, 3, 3)\n",
        "    attn_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
        "\n",
        "    attn_scores.masked_fill_(\n",
        "        self.simple_mask.bool()[:num_tokens, :num_tokens],\n",
        "        -torch.inf\n",
        "    )\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)    ## Shape: (2, 2, 3, 3)\n",
        "\n",
        "    attn_weights = self.dropout(attn_weights)    ## Shape: (2, 2, 3, 3)\n",
        "\n",
        "    ## Calculate context vector\n",
        "    ## (batch, num_heads, num_tokens, num_tokens) * (batch, num_heads, num_tokens, head_dim) = (batch, num_heads, num_tokens, head_dim)\n",
        "    ## Shape: (2, 2, 3, 3) * (2, 2, 3, 3) = (2, 2, 3, 3)\n",
        "    context_vec = torch.matmul(attn_weights, values)    ## Shape: (2, 2, 3, 3)\n",
        "\n",
        "    ## (batch, num_heads, num_tokens, head_dim) -> (batch, num_tokens, num_heads, head_dim)\n",
        "    ## Shape: (2, 2, 3, 3) -> ## Shape: (2, 3, 2, 3)\n",
        "    context_vec = context_vec.transpose(1, 2)    ## Shape: (2, 3, 2, 3)\n",
        "\n",
        "    ## (batch, num_tokens, num_heads, head_dim) -> (batch, num_tokens, d_out)\n",
        "    ## Shape: (2, 3, 2, 3) -> (2, 3, 6)\n",
        "    context_vec = context_vec.contiguous().view(batch, num_tokens, self.d_out)\n",
        "\n",
        "    context_vec = self.out_proj(context_vec)    ## Shape: (2, 3, 6) * (6, 6) = (2, 3, 6)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "jm4aWaEMsiot"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input = torch.tensor(\n",
        "    [[0.72, 0.45, 0.31, 0.30, 0.80, 0.40],   ## The\n",
        "     [0.75, 0.20, 0.55, 0.85, 0.35, 0.60],   ## cat\n",
        "     [0.85, 0.35, 0.60, 0.20, 0.20, 0.85]],   ## sleeps\n",
        ")\n",
        "\n",
        "words = [\"The\", \"cat\", \"sleeps\"]\n",
        "\n",
        "batch = torch.stack([input, input], dim=0)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "dropout = 0.5\n",
        "qkv_bias = True\n",
        "num_heads = 2"
      ],
      "metadata": {
        "id": "_90LMF6-BOhi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
        "context_vector = mha(batch)"
      ],
      "metadata": {
        "id": "q51Jn3GKNXua"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Context Vector:\")\n",
        "print(context_vector)\n",
        "print(\"Shape:\", context_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziBOZnSEBMIa",
        "outputId": "80e6b888-82fd-448e-b406-9125bd5d5b4b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[[ 0.1190,  0.0905,  0.4936, -0.1211,  0.3983,  0.4535],\n",
            "         [ 0.0019,  0.1892,  0.3212, -0.0644,  0.4177,  0.4157],\n",
            "         [-0.0508,  0.3787, -0.2495, -0.2502,  0.2671,  0.1896]],\n",
            "\n",
            "        [[ 0.0037,  0.1198,  0.3855, -0.0577,  0.3625,  0.4866],\n",
            "         [-0.0548,  0.4741, -0.3064, -0.2718,  0.3267,  0.1731],\n",
            "         [-0.0152,  0.0706,  0.3384, -0.0456,  0.4024,  0.3815]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "Shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emXKPLoYOdGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input = torch.tensor(\n",
        "    [[0.72, 0.45, 0.31, 0.30, 0.80, 0.40],   ## The\n",
        "     [0.75, 0.20, 0.55, 0.85, 0.35, 0.60],   ## cat\n",
        "     [0.85, 0.35, 0.60, 0.20, 0.20, 0.85]],   ## sleeps\n",
        ")\n",
        "\n",
        "words = [\"The\", \"cat\", \"sleeps\"]\n",
        "\n",
        "batch = torch.stack([input, input, input, input], dim=0)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 10\n",
        "num_heads = 5\n",
        "dropout = 0.5\n",
        "qkv_bias = True"
      ],
      "metadata": {
        "id": "mrE-DktjBJza"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
        "context_vector = mha(batch)"
      ],
      "metadata": {
        "id": "aLIBoTC3A8nY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Context Vector:\")\n",
        "print(context_vector)\n",
        "print(\"Shape:\", context_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKxE1SpMOyRq",
        "outputId": "0884d668-ae50-499a-8979-3b4c53407b14"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[[ 2.4097e-01,  1.4449e-01,  4.9991e-01,  3.3799e-01,  3.1057e-01,\n",
            "          -1.6071e-01, -4.1381e-01, -2.3795e-01, -6.3857e-01,  3.1919e-01],\n",
            "         [ 4.1329e-01,  1.0487e-01, -1.5155e-01,  9.6190e-02, -4.1447e-01,\n",
            "          -2.0289e-01, -6.8089e-01, -3.3254e-02,  1.9961e-01,  3.4122e-01],\n",
            "         [ 4.0597e-01, -1.2724e-01, -2.2650e-01,  1.1021e-01, -2.1889e-01,\n",
            "           7.1342e-04, -6.4909e-01, -3.0321e-02, -8.3146e-03,  2.7135e-01]],\n",
            "\n",
            "        [[-4.0709e-01, -4.0353e-01, -2.7830e-01, -4.5873e-01, -5.8114e-01,\n",
            "          -1.5919e-01, -7.2865e-01, -9.0930e-01, -2.2909e-01,  4.7108e-01],\n",
            "         [ 4.0491e-01,  2.2273e-01,  1.4551e-01,  4.3604e-01,  2.4016e-01,\n",
            "           3.8988e-01, -5.3599e-01,  2.0897e-02, -4.0695e-01, -4.8698e-02],\n",
            "         [-4.8674e-02, -3.9473e-01,  1.3517e-01, -1.8337e-01,  3.0090e-02,\n",
            "           7.9319e-02, -4.5546e-01, -4.3734e-01, -4.7502e-01,  2.8718e-01]],\n",
            "\n",
            "        [[ 3.5688e-01, -1.7334e-01, -3.4333e-01,  5.5416e-01, -3.1960e-02,\n",
            "          -2.4348e-01, -1.0889e+00, -4.6879e-01, -4.8264e-01,  3.9599e-01],\n",
            "         [-6.6051e-02, -4.4228e-01,  3.1636e-02, -7.2621e-02,  1.3820e-03,\n",
            "          -3.7772e-01, -6.9126e-01, -7.9757e-01, -7.0050e-01,  5.8925e-01],\n",
            "         [ 3.1847e-01,  1.6407e-03,  7.7600e-02,  1.5639e-01, -2.8108e-02,\n",
            "           5.6918e-02, -5.6458e-01, -1.9938e-02, -1.9825e-01,  1.9801e-01]],\n",
            "\n",
            "        [[-4.0709e-01, -4.0353e-01, -2.7830e-01, -4.5873e-01, -5.8114e-01,\n",
            "          -1.5919e-01, -7.2865e-01, -9.0930e-01, -2.2909e-01,  4.7108e-01],\n",
            "         [ 1.6914e-01, -3.5111e-01, -7.8020e-03,  7.3899e-02, -1.1585e-01,\n",
            "          -5.0884e-01, -8.0547e-01, -5.4895e-01, -4.7118e-01,  5.9795e-01],\n",
            "         [-4.1399e-02, -3.1502e-01, -9.0795e-02, -2.8927e-01, -4.7601e-01,\n",
            "          -4.3590e-01, -6.8589e-01, -5.8847e-01, -1.3989e-01,  5.8126e-01]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "Shape: torch.Size([4, 3, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boB1FGX9sij1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}